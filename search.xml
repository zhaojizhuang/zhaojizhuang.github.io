<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【Knative系列】理解 Knative 扩缩容系统的设计]]></title>
    <url>%2F2020%2F07%2F30%2Fknative-autoscalling%2F</url>
    <content type="text"><![CDATA[本文主要讲解 Knative 扩缩容系统的设计原理及实现细节，主要从以下三个方面进行讲解： Knative Serving 扩缩容系统的组件； 涉及的API； 扩缩容和冷启动时的 控制流和数据流的一些细节； 组件Knative Serving 是 Knative 系统的核心，而理解 Knative Serving 系统内的组件能更容易了理解 Knative Serving 系统的实现：了解其中的控制流和数据流的走向，了解其在扩缩容过程中的作用。因篇幅有限，这里只对组件进行简要描述，后续会针对每个组件进行详细的单独讲解。 1. queue-proxyqueue-proxy 是 一个伴随着用户容器运行的 Sidecar 容器，跟用户容器运行在同一个 Pod 中。每个请求到达业务容器之前都会经过 queue-proxy 容器，这也是它问什么叫 proxy 的原因。 queue-proxy 的主要作用是统计和限制到达业务容器的请求并发量，当对一个 Revision 设置了并发量之后（比如设置了5），queue-proxy 会确保不会同时有超过5个请求打到业务容器。当有超过5个请求到来时，queue-proxy会先把请求暂存在自己的队列 queue 里，（这也是为什么名字里有个 queue的缘故）。queue-proxy 同时会统计进来的请求量，同时会通过指定端口提供平均并发量和 rps（每秒请求量）的查询。 2. AutoscalerAutoscaler 是 Knative Serving 系统中一个重要的 pod，它由三部分组成： PodAutoscaler reconciler Collector Decider PodAutoscaler reconciler 会监测 PodAutoscaler（KPA）的变更，然后交由 Collector 和 Decider 处理 Collector 主要负责从应用的 queue-proxy 那里收集指标， Collector 会收集每个实例的指标，然后汇总得到整个系统的指标。为了实现扩缩容，会搜集所有应用实例的样本，并将收集到的样本反映到整个集群。 Decider 得到指标之后，来决定多少个Pod 被扩容出来。简单的计算公式如下： want = concurrencyInSystem/targetConcurrencyPerInstance 另外，扩缩容的量也会受到 Revision 中最大最小实例数的限制。同时 Autoscaler 还会计算当前系统中剩余多少突发请求容量（可扩缩容多少实例）进来决定 请求是否走 Activator 转发。 3. ActivatorActivator 是整个系统中所用应用共享的一个组件，是可以扩缩容的，主要目的是缓存请求并给 Autoscaler主动上报请求指标 Activator 主要作用在从零启动和缩容到零的过程，能根据请求量来对请求进行负载均衡。当 revision 缩容到零之后，请求先经过 Activator 而不是直接到 revision。 当请求到达时，Activator会缓存这这些请求，同时携带请求指标（请求并发数）去触发 Autoscaler扩容实例，当实例 ready后，Activator 才会将请求从缓存中取出来转发出去。同时为了避免后端的实例过载，Activator 还会充当一个负载均衡器的作用，根据请求量决定转发到哪个实例（通过将请求分发到后端所有的Pod上，而不是他们超过设置的负载并发量）。 Knative Serving 会根据不同的情况来决定是否让请求经过 Activator，当一个应用系统中有足够多的pod实例时，Activator 将不再担任代理转发角色，请求会直接打到 revision 来降低网络性能开销。 跟 queue-proxy 不同，Activator 是通过 websocket 主动上报指标给 Autoscaler，这种设计当然是为了应用实例尽可能快的冷启动。queue-proxy 是被动的拉取：Autoscaler去 queue-proxy指定端口拉取指标。 APIPodAutoscaler (PA,KPA)API: podautoscalers.autoscaling.internal.knative.dev PodAutoscaler 是对扩缩容的一个抽象，简写是 KPA 或 PA ，每个 revision 会对应生成一个 PodAutoscaler。 可通过下面的指令查看 kubectl get kpa -n xxx ServerlessServices (SKS)API: serverlessservices.networking.internal.knative.dev ServerlessServices 是 KPA 产生的，一个 KPA 生成一个 SKS，SKS 是对 k8s service 之上的一个抽象， 主要是用来控制数据流是直接流向服务 revision（实例数不为零） 还是经过 Activator（实例数为0）。 对于每个 revision，会对应生成两个k8s service ，一个public service，一个 private service. private service 是标准的 k8s service，通过label selector 来筛选对应的deploy 产生的pod，即 svc 对应的 endpoints 由 k8s 自动管控。 public service 是不受 k8s 管控的，它没有 label selector，不会像 private service 一样 自动生成 endpoints。public service 对应的 endpoints 由 Knative SKS reconciler 来控制。 SKS 有两种模式：proxy 和 serve serve 模式下 public service 后端 endpoints 跟 private service一样， 所有流量都会直接指向 revision 对应的 pod。 proxy 模式下 public service 后端 endpoints 指向的是 系统中 Activator 对应的 pod，所有流量都会流经 Activator。 数据流下面看几种情况下的数据流向，加深对Knative 扩缩容系统机制的理解。 1. 稳定状态下的扩缩容 稳定状态下， Autoscaler 会定期通过 queue-proxy 获取 revision 活跃实例的指标，并不断调整 revision 实例。请求打到系统时， Autoscaler 会根据当前最新的请求指标确定扩缩容比例。 SKS 会监控 private service 的状态，保持 public service 的 endpoints 与 private service 一致 。 2. 缩容到零 一旦系统中某个 revision 不再接收到请求，那么这个 revision 对应的实例数就会缩容到零，此时 Activator 和 queue-proxy 收到的请求数都为 0。 在系统删掉 revision 最后一个 Pod 之前，会先将 Activator 加到 数据流路径中（请求先到 Activator）。 Autoscaler 触发 SKS 变为 proxy 模式，所有的流量都直接导到 Activator， 此时 SKS 的 public service 后端的endpoints 变为 Activator 的IP，此时，如果在冷却窗口时间内没有流量进来，那么最后一个 Pod 才会真正缩容到零。 3. 冷启动（从零开始扩容） 当 revision 缩容到零之后，此时如果有请求进来，则系统需要扩容。因为 SKS 在 proxy 模式，流量会直接请求到 Activator 。Activator 会统计请求量并将 指标主动上报到 Autoscaler，同时 Activator 会缓存请求，并 watch SKS 的 private service， 直到 private service 对应的endpoints产生。 Autoscaler 收到 Activator 发送的指标后，会立即启动扩容的逻辑。这个过程的得出的结论是至少一个Pod要被创造出来，AutoScaler 会修改 revision 对应 Deployment 的副本数为为N（N&gt;0）,AutoScaler同时会将 SKS 的状态置为 serve 模式，流量会直接到导到 revision 对应的 pod上。 Activator 最终会监测到 private service 对应的endpoints的产生，并对 endpoints 进行健康检查。健康检查通过后，Activator 会将之前缓存的请求转发到健康的实例上。 最终 revison 完成了冷启动（从零扩容）。]]></content>
      <categories>
        <category>cilium</category>
      </categories>
      <tags>
        <tag>knative</tag>
        <tag>autoscalling</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自行配置你的HPA扩缩容速率]]></title>
    <url>%2F2020%2F05%2F19%2Fconfighpa%2F</url>
    <content type="text"><![CDATA[HPA介绍HPA, Pod 水平自动伸缩（Horizontal Pod Autoscaler）特性， 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量，（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标custom metrics。 pod 自动缩放不适用于无法缩放的对象，比如 DaemonSets。 Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。 官网文档 HPA 工作机制 关于 HPA 的原理可以看下 baxiaoshi的云原生学习笔记 https://www.yuque.com/baxiaoshi/tyado3/yw9deb,本文不做过多介绍，只介绍自行配置hpa特性 HPA 是由 hpacontroller 来实现的, 通过 --horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒） 一个HPA的例子如下： apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: php-apachespec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: php-apache minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50status: observedGeneration: 1 lastScaleTime: &lt;some-time&gt; currentReplicas: 1 desiredReplicas: 1 currentMetrics: - type: Resource resource: name: cpu current: averageUtilization: 0 averageValue: 0 背景前面已经介绍了 HPA 的相关信息，相信大家都对 HPA 有了简单的了解, 当使用 Horizontal Pod Autoscaler 管理一组副本缩放时， 有可能因为指标动态的变化造成副本数量频繁的变化，有时这被称为 抖动。为了避免这种情况，社区引入了 延迟时间的设置，该配置是针对整个集群的配置，不能对应用粒度进行配置 --horizontal-pod-autoscaler-downscale-stabilization: 这个 kube-controller-manager 的参数表示缩容冷却时间。 即自从上次缩容执行结束后，多久可以再次执行缩容，默认时间是5分钟(5m0s)。` 注意：这个配置是集群级别的，只能配置整个集群的扩缩容速率，用户不能自行配置自己的应用扩缩容速率： 考虑一下场景的应用： 对于大流量的的web应用。需要非常快的扩容速率，缓慢的缩容速率（为了迎接下一个流量高峰） 对于处理数据类的应用。需要尽快的扩容（减少处理数据的时间），并尽快缩容（降低成本） 对于处理常规流量和数据的应用，按照常规的方式配置就可以了 对于上述3种应用场景，1.17以前的集群是不能支持，这也导致了一些未解决的 issue： https://github.com/kubernetes/kubernetes/issues/39090 https://github.com/kubernetes/kubernetes/issues/65097 https://github.com/kubernetes/kubernetes/issues/69428 为此社区 在1.18引入了 能自行配置HPA速率的新特性 见 相关 keps 原理介绍为了自定义扩缩容行为，需要给单个 hpa 对象添加一个 behavior字段，如下： apiVersion: autoscaling/v2beta2kind: HorizontalPodAutoscalermetadata: name: php-apachespec: *** behavior: scaleUp: policies: - type: percent value: 900% scaleDown: policies: - type: pods value: 1 periodSeconds: 600 # (i.e., scale down one pod every 10 min) metrics: *** 其中 behavior 字段 包含 如下对象： scaleUp 配置扩容时的规则。 stabilizationWindowSeconds： -该值表示 HPA 控制器应参考的采样时间，以防止副本数量波动。 selectPolicy：可以是min或max,用于选择 policies 中的最大值还是最小值。默认为 max。 policies： 是一个数组，每个元素有如下字段： type： 可为pods或者percent（以Pod的绝对数量或当前副本的百分比表示）。 periodSeconds 规则应用的时间范围（以秒为单位），即每多少秒扩缩容一次。 value: 规则的值，设为0 表示禁止 扩容或者缩容 scaleDown 与 scaleUp 类似，指定的是缩容的规则。 用户通过控制 HPA中指定参数，来控制HPA扩缩容的逻辑 selectPolicy 字段指示应应用的策略。默认情况下为max，也就是：将使用尽可能扩大副本的最大数量，而选择尽可能减少副本的最小数量。 有点绕没关系，待会看下面的例子 场景场景1：尽可能快的扩容 这种模式适合流量增速比较快的场景 HPA的配置如下： behavior: scaleUp: policies: - type: percent value: 900% 900% 表示可以扩容的数量为当前副本数量的9倍，也就是可以扩大到当前副本数量的10倍，其他值都是默认值 如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下： 1 -&gt; 10 -&gt; 100 -&gt; 1000 scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的扩缩容算法 场景2：尽可能快的扩容，然后慢慢缩容 这种模式适合不想快速缩容的场景 HPA的配置如下： behavior: scaleUp: policies: - type: percent value: 900% scaleDown: policies: - type: pods value: 1 periodSeconds: 600 # (i.e., 每隔10分钟缩容一个pod) 这种配置扩容场景同 场景1，缩容场景下却是不同的，这里配置的是每隔10分钟缩容一个pod 假如说扩容之后有1000个pod，那么缩容过程中pod数量如下： 1000 -&gt; 1000 -&gt; 1000 -&gt; … (7 more min) -&gt; 999 场景3：慢慢扩容，按常规缩容 这种模式适合不太激进的扩容 HPA的配置如下： behavior: scaleUp: policies: - type: pods value: 1 如果应用一个开始的pod数为1，那么扩容过程中 pod数量如下： 1 -&gt; 2 -&gt; 3 -&gt; 4 同样，scaleDown没有配置，表示该应用缩容将按照常规方式进行 可参考对应的扩缩容算法 场景4： 常规扩容，禁止缩容 这种模式适用于 不允许应用缩容，或者你想单独控制应用的缩容的场景 HPA的配置如下： behavior: scaleDown: policies: - type: pods value: 0 副本数量将按照常规方式扩容，不会发生缩容 场景5：延迟缩容 这种模式适用于 用户并不想立即缩容，而是想等待更大的负载时间到来，再计算缩容情况 behavior: scaleDown: stabilizationWindowSeconds: 600 policies: - type: pods value: 5 这种配置情况下 hpa 缩容策略行为如下： 会采集最近 600s 的（默认300s）的缩容建议，类似于滑动窗口 选择最大的那个 按照不大于每秒5个pod的速率缩容 假如 CurReplicas = 10 , HPA controller 每 1min 处理一次: 前 9 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议： recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9] 第 10 min，我们增加一个扩缩容建议，比如说是 8 recommendations = [10, 9, 8, 9, 9, 8, 9, 8, 9，8] HPA 算法会取其中最大的一个 10，因此应用不会发生缩容，repicas 的值不变 第 11 min，我们增加一个扩缩容建议，比如 7，维持 600s 的滑动窗口，因此需要把第一个 10 去掉，如下： recommendations = [9, 8, 9, 9, 8, 9, 8, 9, 8, 7] HPA 算法会取最大的一个 9, 应用副本数量变化： 10 -&gt; 9 场景6： 避免错误的扩容 这种模式在数据处理流中比较常见，用户想要根据队列中的数据来扩容，当数据较多时快速扩容。当指标有抖动时并不想扩容 HPA的配置如下： behavior: scaleUp: stabilizationWindowSeconds: 300 policies: - type: pods value: 20 这种配置情况下 hpa 扩容策略行为如下： 会采集最近 300s 的（默认0s）的扩容建议，类似于滑动窗口 选择最小的那个 按照不大于每秒20个pod的速率扩容 假如 CurReplicas = 2 , HPA controller 每 1min 处理一次: 前 5 min,算法只会收集扩缩容建议，而不会发生真正的扩缩容，假设 有如下 扩缩容建议： recommendations = [2, 3, 19, 10, 3] 第 6 min，我们增加一个扩缩容建议，比如说是 4 recommendations = [2, 3, 19, 10, 3, 4] HPA 算法会取其中最小的一个 2，因此应用不会发生缩容，repicas 的值不变 第 11 min，我们增加一个扩缩容建议，比如 7，维持 300s 的滑动窗口，因此需要把第一个 2 去掉，如下： recommendations = [3, 19, 10, 3, 4，7] HPA 算法会取最小的一个 3, 应用副本数量变化： 2 -&gt; 3 算法原理算法的伪代码如下： // HPA controller 中的for循环for &#123; desiredReplicas = AnyAlgorithmInHPAController(...) // 扩容场景 if desiredReplicas &gt; curReplicas &#123; replicas = []int&#123;&#125; for _, policy := range behavior.ScaleUp.Policies &#123; if policy.type == "pods" &#123; replicas = append(replicas, CurReplicas + policy.Value) &#125; else if policy.type == "percent" &#123; replicas = append(replicas, CurReplicas * (1 + policy.Value/100)) &#125; &#125; if behavior.ScaleUp.selectPolicy == "max" &#123; scaleUpLimit = max(replicas) &#125; else &#123; scaleUpLimit = min(replicas) &#125; // 这里的min可以理解为 尽量维持原状，即如果是扩容，尽量取最小的那个 limitedReplicas = min(max, desiredReplicas) &#125; // 缩容场景 if desiredReplicas &lt; curReplicas &#123; for _, policy := range behaviro.scaleDown.Policies &#123; replicas = []int&#123;&#125; if policy.type == "pods" &#123; replicas = append(replicas, CurReplicas - policy.Value) &#125; else if policy.type == "percent" &#123; replicas = append(replicas, CurReplicas * (1 - policy.Value /100)) &#125; if behavior.ScaleDown.SelectPolicy == "max" &#123; scaleDownLimit = min(replicas) &#125; else &#123; scaleDownLimit = max(replicas) &#125; // 这里的max可以理解为 尽量维持原状，即如果是缩容，尽量取最大的那个 limitedReplicas = max(min, desiredReplicas) &#125; &#125; storeRecommend(limitedReplicas, scaleRecommendations) // 选择合适的扩缩容副本 nextReplicas := applyRecommendationIfNeeded(scaleRecommendations) // 扩缩容 setReplicas(nextReplicas) sleep(ControllerSleepTime) &#125; 默认值为了平滑得扩速容，默认值是有必要的behavior 的默认值如下： behavior.scaleDown.stabilizationWindowSeconds = 300, 缩容情况下默认等待 5 min中再缩容. behavior.scaleUp.stabilizationWindowSeconds = 0, 扩容时立即扩容，不等待 behavior.scaleUp.policies 默认值如下： 百分比策略 policy = percent periodSeconds = 60, 扩容间隔为 1min value = 100 每次最多扩容翻倍 Pod个数策略 policy = pods periodSeconds = 60, 扩容间隔为 1min value = 4 每次最多扩容4个 behavior.scaleDown.policies 默认值如下： 百分比策略 policy = percent periodSeconds = 60 缩容间隔为 1min value = 100 一次缩容最多可以把所有的示例都干掉]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 内存管理]]></title>
    <url>%2F2019%2F11%2F12%2Fneicunguanliyufenpei%2F</url>
    <content type="text"><![CDATA[Go这门语言抛弃了C/C++中的开发者管理内存的方式：主动申请与主动释放，增加了逃逸分析和GC，将开发者从内存管理中释放出来，让开发者有更多的精力去关注软件设计，而不是底层的内存问题。这是Go语言成为高生产力语言的原因之一 引自【 Go内存分配那些事，就这么简单！】 堆内存的分配先看下面这段代码，思考下 smallStruct 会被分配在堆上还是栈上: package maintype smallStruct struct &#123; a, b int64 c, d float64&#125;func main() &#123; smallAllocation()&#125;//go:noinlinefunc smallAllocation() *smallStruct &#123; return &amp;smallStruct&#123;&#125;&#125; 通过 annotation //go:noinline 禁用内联函数，不然这里不会产生堆内存的分配 【逃逸分析】 Inline 内联: 是在编译期间发生的，将函数调用调用处替换为被调用函数主体的一种编译器优化手段。 将文件保存为 main.go, 并执行 go tool compile &quot;-m&quot; main.go ,查看Go 堆内存的分配 【逃逸分析】的过程 如果不加 annotation //go:noinline 可以用go build -gcflags &#39;-m -l&#39; main.go -l可以禁止内联函数，效果是一样的，下面是逃逸分析的结果： main.go:14:9: &amp;smallStruct literal escapes to heap 再来看这段代码生成的汇编指令来详细的展示内存分配的过程, 执行下面 go tool compile -S main.go 0x001d 00029 (main.go:14) LEAQ type."".smallStruct(SB), AX0x0024 00036 (main.go:14) PCDATA $0, $00x0024 00036 (main.go:14) MOVQ AX, (SP)0x0028 00040 (main.go:14) CALL runtime.newobject(SB) runtime.newobject 是 Go 内置的申请堆内存的函数，对于堆内存的分配，Go 中有两种策略: 大内存的分配和小内存的分配 小内存的分配从 P 的 mcache 中分配对于小于 32kb的小内存，Go 会尝试在 P 的 本地缓存 mcache 中分配, mcache 保存的是各种大小的Span，并按Span class分类，小对象直接从mcache分配内存，它起到了缓存的作用，并且可以 无锁访问 每个 M 绑定一个 P 来运行一个goroutine, 在分配内存时，当前的 goroutine 在当前 P 的本地缓存 mcache 中查找对应的span， 从 span list 中来查找第一个可用的空闲 span span class 分为 8 bytes ~ 32k bytes 共 66 种类型（还有个大小为0的 size class0，并未用到，用于大对象的堆内存分配），分别对应不同的内存大小,mspan里保存对应大小的object， 1个 size class 对应2个 span class，2个 span class 的 span 大小相同，只是功能不同，1个用来存放包含指针的对象，一个用来存放不包含指针的对象，不包含指针对象的 Span 就无需 GC 扫描了。 前面的例子里，struct的大小为 (64bit/8)*4=8bit*4=32b 所以会在 span class 大小为 32bytes 的 mspan 里分配 从全局的缓存 mcentral 中分配当 mcache中没有空闲的 span 时怎么办呢，Go 还维护了一个全局的缓存 mcentral, mcentral 和 mcache 一样，都134个 span class 级别(67个 size class )，但每个级别都保存了2个span list，即2个span链表： nonempty：这个链表里的span，所有span都至少有1个空闲的对象空间。这些span是mcache释放span时加入到该链表的。 empty：这个链表里的span，所有的span都不确定里面是否有空闲的对象空间。当一个span交给mcache的时候，就会加入到empty链表。 mcache从mcentral获取和归还mspan的流程：引自【 图解Go语言内存分配|码农桃花源】 获取：加锁；从 nonempty 链表找到一个可用的 mspan ；并将其从 nonempty 链表删除；将取出的 mspan 加入到 empty 链表；将 mspan 返回给工作线程；解锁。 归还：加锁；将 mspan 从 empty 链表删除；将 mspan 加入到 nonempty 链表；解锁。 另外，GC 扫描的时候会把部分 mspan 标记为未使用，并将对应的 mspan 加入到 nonempty list 中 mcache 从 mcentral获取 过程如下： mcentral 从 heap 中分配当 mcentral 中的 nonempty list 没有可分配的对象的时候，Go会从 mheap 中分配对象，并链接到 nonempty list 上, mheap 必要时会向系统申请内存 mheap 中还有 arenas ,主要是为了大块内存需要，arena 也是用 mspan 组织的 大内存的分配大内存的分配就比较简单了，大于 32kb 的内存都会在 mheap 中直接分配 总结go 内存分配的概览 参考文章 Go内存分配那些事，就这么简单！:https://lessisbetter.site/2019/07/06/go-memory-allocation 图解Go语言内存分配|码农桃花源:https://qcrao.com/2019/03/13/graphic-go-memory-allocation 聊一聊goroutine stack：https://zhuanlan.zhihu.com/p/28409657]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈 epoll]]></title>
    <url>%2F2019%2F05%2F10%2Fepoll%2F</url>
    <content type="text"><![CDATA[epollIO 多路复用目前支持I/O多路复用的系统调用有 select，pselect，poll，epoll ，I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作 select调用后select函数会阻塞，直到有描述符就绪（有数据可读、可写），或者超时，函数返回。当select函数返回后，可以通过遍历fdset，来找到就绪的描述符。 select的流程 假如程序同时监视如下图的sock1、sock2和sock3三个socket，那么在调用select之后，操作系统把进程A分别加入这三个socket的等待队列中 当任何一个socket收到数据后，中断程序将唤起进程,将进程从所有fd（socket）的等待队列中移除，再将进程加入到工作队列里面 进程A被唤醒后，它知道至少有一个socket接收了数据。程序需遍历一遍socket列表，可以得到就绪的socket 缺点： 其一，每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及了两次遍历，而且每次都要将整个fds列表传递给内核，有一定的开销。正是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。 其二，进程被唤醒后，程序并不知道哪些socket收到数据，还需要遍历一次。 poll与select一样，只是去掉了 1024的限制epollepoll 事先通过 epoll_ctl() 来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似 callback 的回调机制，迅速激活这个文件描述符，当进程调用 epoll_wait() 时便得到通知。(此处去掉了遍历文件描述符，而是通过监听回调的的机制。这正是epoll的魅力所在。) epoll使用一个文件描述符(eventpoll)管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次 int s = socket(AF_INET, SOCK_STREAM, 0); bind(s, ...)listen(s, ...)int epfd = epoll_create(...);epoll_ctl(epfd, ...); //将所有需要监听的socket添加到epfd中while(1)&#123; int n = epoll_wait(...) for(接收到数据的socket)&#123; //处理 &#125;&#125; 流程：首先创建 epoll对象创建epoll对象后，可以用epoll_ctl添加或删除所要监听的socket 假设计算机中正在运行进程A和进程B，在某时刻进程A运行到了epoll_wait语句。如下图所示，内核会将进程A放入eventpoll的等待队列中，阻塞进程 当socket接收到数据，中断程序一方面修改rdlist，另一方面唤醒eventpoll等待队列中的进程，进程A再次进入运行状态（如下图）。也因为rdlist的存在，进程A可以知道哪些socket发生了变化。 参考文章 https://www.jianshu.com/p/dfd940e7fca2 2 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(1) 3 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(2) 4 罗培羽：如果这篇文章说不清epoll的本质，那就过来掐死我吧！(3)]]></content>
      <tags>
        <tag>linux</tag>
        <tag>epoll</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[浅谈 Go map]]></title>
    <url>%2F2019%2F04%2F02%2Fgomap%2F</url>
    <content type="text"><![CDATA[map原理分析map 结构体type hmap struct &#123; count int // 元素的个数 flags uint8 // 状态标志 B uint8 // 可以最多容纳 6.5 * 2 ^ B 个元素，6.5为装载因子 noverflow uint16 // 溢出的个数 hash0 uint32 // 哈希种子 buckets unsafe.Pointer // 桶的地址 oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容 nevacuate uintptr // 搬迁进度，小于nevacuate的已经搬迁 overflow *[2]*[]*bmap &#125;// A bucket for a Go map.type bmap struct &#123; // 每个元素hash值的高8位，如果tophash[0] &lt; minTopHash，表示这个桶的搬迁状态 tophash [bucketCnt]uint8 // bucketCnt是常量8,接下来是8个key、8个value，但是我们不能直接看到；为了优化对齐，go采用了key放在一起，value放在一起的存储方式，8个k，8个v得内存地址 // 再接下来是hash冲突发生时，下一个溢出桶的地址&#125; bmap不只tophash还有两个方法 overflow 和setoverflow func (b *bmap) overflow(t *maptype) *bmap &#123; return *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize))&#125;func (b *bmap) setoverflow(t *maptype, ovf *bmap) &#123; *(**bmap)(add(unsafe.Pointer(b), uintptr(t.bucketsize)-sys.PtrSize)) = ovf&#125; hmap中的buckets中的原色bucket就是bmap，即 buckets[0],bucket[1],… bucket[2^B-1]如下图 bucket就是bmap bmap 是存放 k-v 的地方，我们把视角拉近，仔细看 bmap 的内部组成。 key 经过哈希计算后得到哈希值，共 64 个 bit 位（64位机，32位机就不讨论了，现在主流都是64位机），计算它到底要落在哪个桶时，只会用到最后 B 个 bit 位。还记得前面提到过的 B 吗？如果 B = 5，那么桶的数量，也就是 buckets 数组的长度是 2^5 = 32 例如，现在有一个 key 经过哈希函数计算后，得到的哈希结果是： 10010111 | 000011110110110010001111001010100010010110010101010 │ 01010 用最后的 5 个 bit 位，也就是 01010，值为 10，也就是 10 号桶。这个操作实际上就是取余操作，但是取余开销太大，所以代码实现上用的位操作代替。 再用哈希值的高 8 位，找到此 key 在 bucket 中的位置，这是在寻找已有的 key。最开始桶内还没有 key，新加入的 key 会找到第一个空位，放入。 buckets 编号就是桶编号，当两个不同的 key 落在同一个桶中，也就是发生了哈希冲突。冲突的解决手段是用链表法：在 bucket 中，从前往后找到第一个空位。这样，在查找某个 key 时，先找到对应的桶，再去遍历 bucket 中的 key hash冲突的两种表示方式： 开放寻址法（hash冲突时，在当前index往后查找第一个空的位置即可） 拉链法 map在写入过程会发生扩容，runtime.mapassign 函数会在以下两种情况发生时触发哈希的扩容： 装载因子已经超过 6.5；装载因子=总数量/桶的数量 哈希使用了太多溢出桶；溢出捅的数量 超过正常桶的数量 即 noverflow 大于 1&lt;&lt;B buckets 每次都会将桶的数量翻倍 扩容机制： 翻倍扩容：哈希在存储元素过多时状态会触发扩容操作，每次都会将桶的数量翻倍，整个扩容过程并不是原子的，而是通过 runtime.growWork 增量触发的，在扩容期间访问哈希表时会使用旧桶，向哈希表写入数据时会触发旧桶元素的分流； 等量扩容，为了解决大量写入、删除造成的内存泄漏问题，哈希引入了 sameSizeGrow这一机制，在出现较多溢出桶时会对哈希进行『内存整理』减少对空间的占用。 参考链接 https://www.jianshu.com/p/aa0d4808cbb8 https://segmentfault.com/a/1190000018387055]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go 学习笔记]]></title>
    <url>%2F2019%2F01%2F05%2Fgo%20shen%20ru%20fen%20xi%2F</url>
    <content type="text"><![CDATA[Go 学习笔记go程序是如何运行的参考链接1 defer 源码分析参考链接 defer、return、返回值三者的执行逻辑应该是：return最先执行，return负责将结果写入返回值中；接着defer开始执行一些收尾工作；最后函数携带当前返回值退出 逃逸分析 堆栈分配参考链接 go build -gcflags &#39;-m -l&#39; xxx.go 就可以看到逃逸分析的过程和结果 go性能大杀器 pprof参考链接1参考链接2 ### pprof中自带 web 火焰图，需要安装graphvizgo tool pprof -http=:8181 xxx,pprof 下面的语句 可以结合代码查看哪个函数用时最多go tool pprof main.go xxxx.prof 进入pprof后执行 list &lt;函数名&gt; 对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import,会增加 debug/pprof 的endpoint),结束后将默认进入 pprof 的交互式命令模式go tool pprof http://localhost:6060/debug/pprof/profile?seconds=60go tool pprof http://localhost:6060/debug/pprof/heap go性能大杀器 trace同pprof 对于web开放的pprof （在http的go程序中 添加 _ &quot;net/http/pprof&quot;的import curl http://127.0.0.1:6060/debug/pprof/trace\?seconds\=20 &gt; trace.outgo tool trace trace.out # 此处和pprof不同，不用加 -http=:8181 这里他会自动选择端口 对于后台应用,后台程序main启动时添加 trace.Start(os.Stderr)直接运行下面的命令即可 go run main.go 2&gt; trace.out 它能够跟踪捕获各种执行中的事件，例如 Goroutine 的创建/阻塞/解除阻塞，Syscall 的进入/退出/阻止，GC 事件，Heap 的大小改变，Processor 启动/停止等等 interface 不含有任何方法的 interface type eface struct &#123; // 16 bytes _type *_type data unsafe.Pointer&#125; 含有 方法的 interface type iface struct &#123; // 16 bytes tab *itab data unsafe.Pointer&#125; 变量类型 结构体实现接口 结构体指针实现接口 结构体初始化变量 通过 不通过 结构体指针初始化变量 通过 通过 不通过的如下 type Duck interface &#123; Quack()&#125;type Cat struct&#123;&#125;func (c *Cat) Quack() &#123; fmt.Println("meow")&#125;func main() &#123; var c Duck = Cat&#123;&#125; // 将结构体变量传到指针类型接受的函数是不行的，反过来可行 c.Quack()&#125;$ go build interface.go./interface.go:20:6: cannot use Cat literal (type Cat) as type Duck in assignment: Cat does not implement Duck (Quack method has pointer receiver) Go中函数调用都是值拷贝，使用 c.Quack() 调用方法时都会发生值拷贝： 对于 &amp;Cat{} 来说，这意味着拷贝一个新的 &amp;Cat{} 指针，这个指针与原来的指针指向一个相同并且唯一的结构体，所以编译器可以隐式的对变量解引用（dereference）获取指针指向的结构体； 对于 Cat{} 来说，这意味着 Quack 方法会接受一个全新的 Cat{}，因为方法的参数是*Cat，编译器不会无中生有创建一个新的指针；即使编译器可以创建新指针，这个指针指向的也不是最初调用该方法的结构体； panicpanic只会调用当前Goroutine的defer（） func main() &#123; defer println("in main") go func() &#123; defer println("in goroutine") panic("") &#125;() time.Sleep(1 * time.Second)&#125;$ go run main.goin goroutinepanic: make newnew 返回的是指针，指向一个type类型内存空间的指针new等价于 var a typeA // tpyeA的零值b:=&amp;a 但是 new不能对 chanel map slice进行初始化 ，这几个必须经过make进行结构体的初始化才能用]]></content>
      <categories>
        <category>Go</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>epoll</tag>
        <tag>Go</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes&容器网络（2）之iptables]]></title>
    <url>%2F2018%2F12%2F05%2Fk8s-iptables%2F</url>
    <content type="text"><![CDATA[iptables规则 参考 http://www.zsythink.net/archives/tag/iptables/ 用户空间，例如从pod中流出的流量就是从ouput链流出 上图表示的iptables的链，链 和表的关系如下，以PREROUTING链为例 这幅图是什么意思呢？它的意思是说，prerouting”链”只拥有nat表、raw表和mangle表所对应的功能，所以，prerouting中的规则只能存放于nat表、raw表和mangle表中。 NATNAT的三种类型: SNAT iptables -t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3# 目标流向eth0，源地址是xxx的，做SNAT，源地址改为xxx DNAT iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j SNAT --to-source192.168.5.3-192.168.5.5 MASQUERADE 是SNAT的一种，可以自动获取网卡的ip来做SNAT，如果是ADSL这种动态ip的，如果用SNAT需要经常更改iptables规则 iptables-t nat -A POSTROUTING -s 10.8.0.0/255.255.255.0 -o eth0 -j MASQUERADE# 源地址是xxx，流向eth0的，流向做自动化SNAT masquerade 应为英文伪装 iptabels 常用命令iptables [-t 表名] 管理选项 [链名] [匹配条件] [-j 控制类型]# 控制类型包括 ACCETP REJECT DROP LOG 还有自定义的链（k8s的链）等iptabels -t nat（表名） -nvL POSTROUTING(链的名字) https://www.jianshu.com/p/ee4ee15d3658 分析k8s下的iptables规则以如下 service 为例 Name: testapi-smzdm-comNamespace: zhongce-v2-0Labels: &lt;none&gt;Selector: zdm-app-owner=testapi-smzdm-comType: LoadBalancerIP: 172.17.185.22LoadBalancer Ingress: 10.42.162.216Port: &lt;unset&gt; 809/TCPTargetPort: 809/TCPNodePort: &lt;unset&gt; 39746/TCPEndpoints: 10.42.147.255:809,10.42.38.222:809Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 即 cluster ip 为 172.17.185.22 后端 podip 为 10.42.147.25510.42.38.222此外还有1个 loadbalancer ip 10.42.162.216 svc的访问路径- 集群内部，通过 `clusterip` 到访问到后端 `pod - 集群外部，通过直接访问`nodeport`；或者通过 `elb` 负载均衡到 `node` 上再通过 `nodeport` 访问 cluster ip 的基本原理如果是集群内的应用访问 cluster ip，那就是从用户空间访问内核空间网络协议栈,走的是 OUTPUT 链 从OUTPUT 链开始 [root@10-42-8-102 ~]# iptables -t nat -nvL OUTPUTChain OUTPUT (policy ACCEPT 4 packets, 240 bytes) pkts bytes target prot opt in out source destination 3424K 209M KUBE-SERVICES all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service portals */ OUTPUT下的规则 直接把流量交给 KUBE-SERVICES 链 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICESChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 10 520 KUBE-FW-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 10.42.162.216 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTY 上述3条规则是顺序执行的： 第1条规则匹配发往 Cluster IP 172.17.185.22 的流量，跳转到了 KUBE-MARK-MASQ 链进一步处理，其作用就是打了一个 MARK ，稍后展开说明。 第2条规则匹配发往 Cluster IP 172.17.185.22 的流量，跳转到了 KUBE-SVC-G3OM5DSD2HHDMN6U 链进一步处理，稍后展开说明。 第3条规则匹配发往集群外 LB IP 的 10.42.162.216 的流量，跳转到了KUBE-FW-G3OM5DSD2HHDMN6U 链进一步处理，稍后展开说明。 第4条 KUBE-NODEPORTS的规则在末尾，只要dst ip是node 本机ip的话 （（–dst-type LOCAL），就跳转到KUBE-NODEPORTS做进一步判定：） 第2条规则要做dnat转发到后端具体的后端pod上 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SVC-G3OM5DSD2HHDMN6UChain KUBE-SVC-G3OM5DSD2HHDMN6U (3 references) pkts bytes target prot opt in out source destination 18 936 KUBE-SEP-JT2KW6YUTVPLLGV6 all -- * * 0.0.0.0/0 0.0.0.0/0 statistic mode random probability 0.50000000000 21 1092 KUBE-SEP-VETLC6CJY2HOK3EL all -- * * 0.0.0.0/0 0.0.0.0/0 两条 对应 后端pod的链 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-JT2KW6YUTVPLLGV6Chain KUBE-SEP-JT2KW6YUTVPLLGV6 (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.42.147.255 0.0.0.0/0 26 1352 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp to:10.42.147.255:809[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SEP-VETLC6CJY2HOK3ELChain KUBE-SEP-VETLC6CJY2HOK3EL (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ all -- * * 10.42.38.222 0.0.0.0/0 2 104 DNAT tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp to:10.42.38.222:809 流量经过路由表从eth0出去，在流量流出本机之前会经过POSTROUTING 链 在流量离开本机的时候会经过 POSTROUTING 链[root@10-42-8-102 ~]# iptables -t nat -nvL POSTROUTINGChain POSTROUTING (policy ACCEPT 274 packets, 17340 bytes) pkts bytes target prot opt in out source destination 632M 36G KUBE-POSTROUTING all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes postrouting rules */[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-POSTROUTINGChain KUBE-POSTROUTING (1 references) pkts bytes target prot opt in out source destination 526 27352 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service traffic requiring SNAT */ mark match 0x4000/0x4000 其实直接就跳转到了 KUBE-POSTROUTING，然后匹配打过0x4000 MARK 的流量，将其做 SNAT 转换，而这个 MARK 其实就是之前没说的 KUBE-MARK-MASQ 做的事情 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-MARK-MASQChain KUBE-MARK-MASQ (183 references) pkts bytes target prot opt in out source destination 492 25604 MARK all -- * * 0.0.0.0/0 0.0.0.0/0 MARK or 0x4000 当流量离开本机时，src IP会被修改为node的IP，而不是发出流量的POD IP了 通过loadbalance ip进行访问最后还有一个KUBE-FW-G3OM5DSD2HHDMN6U链没有讲，从本机发往LB IP的流量要做啥事情呢？ 其实也是让流量直接发往具体某个Endpoints，就别真的发往LB了，这样才能获得最佳的延迟：[root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-FW-G3OM5DSD2HHDMN6UChain KUBE-FW-G3OM5DSD2HHDMN6U (1 references) pkts bytes target prot opt in out source destination 2 104 KUBE-MARK-MASQ all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 2 104 KUBE-SVC-G3OM5DSD2HHDMN6U all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 0 0 KUBE-MARK-DROP all -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ 通过nodeport 来访问回顾一下 KUBE_SERVICES规则 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-SERVICESChain KUBE-SERVICES (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * !172.17.0.0/16 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 172.17.185.22 /* zhongce-v2-0/testapi-smzdm-com: cluster IP */ tcp dpt:809 10 520 KUBE-FW-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 10.42.162.216 /* zhongce-v2-0/testapi-smzdm-com: loadbalancer IP */ tcp dpt:809 0 0 KUBE-NODEPORTS all -- * * 0.0.0.0/0 0.0.0.0/0 /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL KUBE-NODEPORTS 是最后一条规则 [root@10-42-8-102 ~]# iptables -t nat -nvL KUBE-NODEPORTSChain KUBE-NODEPORTS (1 references) pkts bytes target prot opt in out source destination 0 0 KUBE-MARK-MASQ tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746 0 0 KUBE-SVC-G3OM5DSD2HHDMN6U tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* zhongce-v2-0/testapi-smzdm-com: */ tcp dpt:39746 第1条匹配dst port如果是39746，那么就打mark。第2条匹配dst port如果是39746，那么就跳到负载均衡链做DNAT改写。 总结 KUBE-SERVICES 链的规则存在于 OUTPUT POSTROUTING PREROUTING 三个链上 对于 KUBE-SERVICES KUBE-NDOEPORTS-xxx KUBE-SEP-xxx 下都会对符合条件（匹配条件）的规则打上MARK 可以重复打MARK 在流量出node的时候做SNAT 从集群内出去的流量怎么回来出node的流量在做SNAT的时候，netfilter有个连接跟踪机制，保存在 conntrack记录中 这就是Netfilter的连接跟踪（conntrack）功能了。对于TCP协议来讲，肯定是上来先建立一个连接，可以用源/目的IP+源/目的端口 （四元组），唯一标识一条连接，这个连接会放在conntrack表里面。当时是这台机器去请求163网站的，虽然源地址已经Snat成公网IP地址了，但是 conntrack 表里面还是有这个连接的记录的。当163网站返回数据的时候，会找到记录，从而找到正确的私网IP地址。 参考文档k8s 的iptales规则详解]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kubernetes&容器网络（1）之seivice]]></title>
    <url>%2F2018%2F11%2F28%2Fk8s-svc%2F</url>
    <content type="text"><![CDATA[1. Service介绍Kubernetes 中有很多概念，例如 ReplicationController、Service、Pod等。我认为 Service 是 Kubernetes 中最重要的概念，没有之一。 为什么 Service 如此重要？因为它解耦了前端用户和后端真正提供服务的 Pods 。在进一步理解 Service 之前，我们先简单地了解下 Pod ， Pod 也是 Kubernetes 中很重要的概念之一。 在 Kubernetes 中，Pod 是能够创建、调度、和管理的最小部署单元，而不是单独的应用容器。Pod 是容器组，一个Pod中容器运行在一个共享的应用上下文中。这里的共享上下文是为多个 Linux Namespace 的联合。例如： PID命名空间（在同一个 Pod 中的应用可以看到其它应用的进程） Network名字空间（在同一个 Pod 中的应用可以访问同样的IP和端口空间） IPC命名空间（在同一个 Pod 中的应用可以使用SystemV IPC或者POSIX消息队列进行通信） UTS命名空间（在同一个 Pod 中的应用可以共享一个主机名称） Pod 是一个和应用相关的“逻辑主机”， Pod 中的容器共享一个网络名字空间。 Pod 为它的组件之间的数据共享、通信和管理提供了便利。 我们可以看出， Pod 在资源层面抽象了容器。 因此，在Kubernetes中，让我们暂时先忘记容器，记住 Pod 。 Kubernetes对Service的定义是：Service is an abstraction which defines a logical set of Pods and a policy by which to access them。我们下面理解下这句话。 刚开始的时候，生活其实是很简单的。一个提供特定服务的进程，运行在一个容器中，监听容器的IP地址和端口号。客户端通过&lt;Container IP&gt;:&lt;ContainerPort&gt;，或者通过使用Docker的端口映射&lt;Host IP&gt;:&lt;Host Port&gt;就可以访问到这个服务了。The simplest, the best，简单的生活很美好。 但是美好的日子总是短暂的。小伙伴们太热情了，单独由一个容器提供服务不够用了，怎么办？很简单啊，由多个容器提供服务不就可以了吗。问题似乎得到了解决。 可是那么多的容器，客户端到底访问哪个容器中提供的服务呢？访问容器的请求不均衡怎么办？假如容器所在的主机故障了，容器在另外一台主机上拉起了，这个时候容器的IP地址变了，客户端怎么维护这个容器列表呢？所以，由多个容器提供服务的情况下，一般有两种做法： 客户端自己维护提供服务的容器列表，自己做负载均衡，某个容器故障时自己做故障转移；提供一个负载均衡器，解耦用户和后端提供服务的容器。负载均衡器负责向后端容器转发流量，并对后端容器进行健康检查。客户端只要访问负载均衡器的IP和端口号就行了。我们在前面说Service解耦了前端用户和后端真正提供服务的 Pod s。从这个意义上讲，Service就是Kubernetes中 Pod 的负载均衡器。 从生命周期来说， Pod 是短暂的而不是长久的应用。 Pod 被调度到节点，保持在这个节点上直到被销毁。当节点死亡时，分配到这个节点的 Pod 将会被删掉。 但Service在其生命周期内，IP地址是稳定的。对于Kubernetes原生的应用，Kubernetes提供了一个Endpoints的对象，这个Endpoints的名字和Service的名字相同，它是一个:的列表，负责维护Service后端的Pods的变化。 总结一下，Service解耦了前端用户和后端真正提供服务的Pods，Pod在资源层面抽象了容器。由于它们的存在，使得这个简单的世界变得复杂了。 对了，Service怎么知道是哪些后端的Pods在真正提供自己定义的服务呢？在创建Pods的时候，会定义一些label；在创建Service的时候，会定义Label Selector，Kubernetes就是通过Label Selector来匹配后端真正服务于Service的后端Pods的。 2. 定义一个Service接下来就有点没意思了，我要开始翻译上面说的很重要的Services in Kubernetes了。当然，我会加入自己的理解。 Service也是Kubernetes中的一个REST对象。可以通过向apiserver发送POST请求进行创建。当然，Kubernetes为我们提供了一个强大和好用的客户端kubectl，代替我们向apiserver发送请求。但是kubectl不仅仅是一个简单的apiserver客户端，它为我们提供了很多额外的功能，例如rolling update等。 我们可以创建一个yaml或者json格式的Service规范文件，然后通过kubectl create -f &lt;service spec file&gt;创建之。一个Service的例子如下： &#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ] &#125;&#125; 以上规范创建了一个名字为my-service的Service对象，它指向任何有app=MyApp标签的、监听TCP端口9376的任何Pods。 Kubernetes会自动地创建一个和Service名字相同的Endpoints对象。Service的selector会被持续地评估哪些Pods属于这个Service，结果会被更新到相应的Endpoints对象。 当然，你也可以在定义Service的时候为其指定一个IP地址（ClusterIP，必须在kube-apiserver的--service-cluster-ip-range参数定义内，且不能冲突）。 Service会把到:的流量转发到targetPort。缺省情况下targetPort等于port的值。一个有意思的情况是，这里你可以定义targetPort为一个字符串（我们可以看到targetPort的类型为IntOrString），这意味着在后端每个Pods中实际监听的端口值可能不一样，这极大地提高了部署Service的灵活性。 Kubernetes的Service支持TCP和UDP协议，缺省是TCP。 3. Service发布服务的方式Service有三种类型：ClusterIP，NodePort和LoadBalancer。 3.1 ClusterIP关于ClusterIP： 通过Service的spec.type: ClusterIP指定；使用cluster-internal ip，即kube-apiserver的--service-cluster-ip-range参数定义的IP范围中的IP地址；缺省方式；只能从集群内访问，访问方式：&lt;ClusterIP&gt;:&lt;Port&gt;；kube-proxy会为每个Service，打开一个本地随机端口，通过iptables规则把到Service的流量trap到这个随机端口，由kube-proxy或者iptables接收，进而转发到后端Pods。 3.2 NodePort关于NodePort： 通过Service的spec.type: NodePort指定；包含ClusterIP功能；在集群的每个节点上为Service开放一个端口（默认是30000-32767，由kube-apiserver的--service-node-port-range参数定义的节点端口范围）；可从集群外部通过&lt;NodeIP&gt;:&lt;NodePort&gt;访问；集群中的每个节点上，都会监听NodePort端口，因此，可以通过访问集群中的任意一个节点访问到服务。 3.3 LoadBalancer关于LoadBalancer： 通过Service的spec.type: LoadBalancer指定；包含NodePort功能；通过Cloud Provider（例如GCE）提供的外部LoadBalancer访问服务，即&lt;LoadBalancerIP&gt;:&lt;Port&gt;；Service通过集群的每个节点上的&lt;NodeIP&gt;:&lt;NodePort&gt;向外暴露；有的cloudprovider支持直接从LoadBalancer转发流量到后端Pods（例如GCE），更多的是转发流量到集群节点（例如AWS，还有HWS）；你可以在Service定义中指定loadBalancerIP，但这需要cloudprovider的支持，如果不支持则忽略。真正的IP在status.loadBalancer.ingress.ip中。一个例子如下： &#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376, "nodePort": 30061 &#125; ], "clusterIP": "10.0.171.239", "loadBalancerIP": "78.11.24.19", "type": "LoadBalancer" &#125;, "status": &#123; "loadBalancer": &#123; "ingress": [ &#123; "ip": "146.148.47.155" &#125; ] &#125; &#125;&#125; 目前对接ELB的实现几大厂商，比如 HW 是ELB转发流量到集群节点，后面再由kube-proxy或者iptables转发到后端的Pods。 3.4 External IPsExternal IPs 不是一种Service类型，它不由Kubernetes管理，但是我们也可以通过它暴露服务。数据包通过&lt;External IP&gt;:&lt;Port&gt;到达集群，然后被路由到Service的Endpoints。一个例子如下： &#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "name": "http", "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ], "externalIPs" : [ "80.11.12.10" ] &#125;&#125; 4. 几种特殊的Service4.1 没有selector的Service上面说Kubernetes的Service抽象了到Kubernetes的Pods的访问。但是它也能抽象到其它类型的后端的访问。举几个场景： 你想接入一个外部的数据库服务；你想把一个服务指向另外一个Namespace或者集群的服务；你把部分负载迁移到Kubernetes，而另外一部分后端服务运行在Kubernetes之外。在这几种情况下，你可以定义一个没有selector的服务。如下： &#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "ports": [ &#123; "protocol": "TCP", "port": 80, "targetPort": 9376 &#125; ] &#125;&#125; 因为没有selector，Kubernetes不会自己创建Endpoints对象，你需要自己手动创建一个Endpoints对象，把Service映射到后端指定的Endpoints上。 &#123; "kind": "Endpoints", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "subsets": [ &#123; "addresses": [ &#123; "IP": "1.2.3.4" &#125; ], "ports": [ &#123; "port": 9376 &#125; ] &#125; ]&#125; 注意：Endpoint IP不能是loopback（127.0.0.1）地址、link-local（169.254.0.0/16）和link-local multicast（224.0.0.0/24）地址。 看到这里，我们似乎明白了，这不就是Kubernetes提供的外部服务接入的方式吗？和CloudFoundry的ServiceBroker的功能类似。 4.2 多端口（multi-port）的ServiceKubernetes的Service还支持多端口，比如同时暴露80和443端口。在这种情况下，你必须为每个端口定义一个名字以示区分。一个例子如下： &#123; "kind": "Service", "apiVersion": "v1", "metadata": &#123; "name": "my-service" &#125;, "spec": &#123; "selector": &#123; "app": "MyApp" &#125;, "ports": [ &#123; "name": "http", "protocol": "TCP", "port": 80, "targetPort": 9376 &#125;, &#123; "name": "https", "protocol": "TCP", "port": 443, "targetPort": 9377 &#125; ] &#125;&#125; 注意： 多端口必须指定ports.name以示区分，端口名称不能一样；如果是spec.type: NodePort，则每个端口的NodePort必须不一样，否则Kubernetes不知道一个NodePort对应的是后端哪个targetPort；协议protocol和port可以一样。 4.3 Headless services有时候你不想或者不需要Kubernetes为你的服务做负载均衡，以及一个Service的IP地址。在这种情况下，你可以创建一个headless的Service，通过指定spec.clusterIP: None。 对这类Service，不会分配ClusterIP。对这类Service的DNS查询会返回一堆A记录，即后端Pods的IP地址。另外，kube-proxy不会处理这类Service，Kubernetes不会对这类Service做负载均衡或者代理。但是Endpoints Controller还是会为此类Service创建Endpoints对象。 这允许开发者减少和kubernetes的耦合性，允许他们自己做服务发现等。我在最后讨论的一些基于Kubernetes的容器服务，除了彻底不使用Service的概念外，也可以创建这类headless的Service，自己直接通过LoadBalancer把流量转发（负载均衡和代理）到后端Pods。 5. Service的流量转发模式5.1 Proxy-mode: userspaceuserspace的代理模式是指由用户态的kube-proxy转发流量到后端Pods。如下图所示。 关于userspace： Kube-proxy通过apiserver监控（watch）Service和Endpoints的变化；Kube-proxy安装iptables规则；Kube-proxy把访问Service的流量转发到后端真正的Pods上（Round-Robin）；Kubernetes v1.0只支持这种转发方式；通过设置service.spec.sessionAffinity: ClientIP支持基于ClientIP的会话亲和性。 5.2 proxy-mode: iptablesiptables的代理模式是指由内核态的iptables转发流量到后端Pods。如下图所示。 关于iptables： Kube-proxy通过apiserver监控（watch）Service和Endpoints的变化； Kube-proxy安装iptables规则； iptables把访问Service的流量转发到后端真正的Pods上（Random）； Kubernetes v1.1已支持，但不是默认方式，v1.2中将会是默认方式； 通过设置service.spec.sessionAffinity: ClientIP支持基于ClientIP的会话亲和性； 需要iptables和内核版本的支持。iptables &gt; 1.4.11，内核支持route_localnet参数(kernel &gt;= 3.6)； 相比userspace的优点： 1，数据包不需要拷贝到用户态的kube-proxy再做转发，因此效率更高、更可靠。 2，不修改Client IP。 5.3 proxy-mode: iptables kubernetes v1.8 引入， 1.11正式可用 在 ipvs 模式下，kube-proxy监视Kubernetes服务和端点，调用 netlink接口相应地创建 IPVS 规则， 并定期将 IPVS 规则与 Kubernetes 服务和端点同步。 该控制循环可确保 IPVS 状态与所需状态匹配。 访问服务时，IPVS 将流量定向到后端Pod之一。 IPVS代理模式基于类似于 iptables模式的 netfilter 挂钩函数，但是使用哈希表作为基础数据结构，并且在内核空间中工作。 这意味着，与 iptables 模式下的 kube-proxy 相比，IPVS 模式下的 kube-proxy 重定向通信的延迟要短，并且在同步代理规则时具有更好的性能。与其他代理模式相比，IPVS 模式还支持更高的网络流量吞吐量。 IPVS提供了更多选项来平衡后端Pod的流量。 这些是： rr: round-robin lc: least connection (最小连接数) dh: destination hashing（目的地址has） sh: source hashing（源地址has） 等等 5.3 userspace和iptables转发方式的主要不同点userspace和iptables转发方式的主要不同点如下： 比较项 userspace iptables 谁转发流量到Pods| kube-proxy把访问Service的流量转发到后端真正的Pods上 |iptables把访问Service的流量转发到后端真正的Pods上|转发算法 |轮询Round-Robin| 随机Random|用户态和内核态 |数据包需要拷贝到用户态的kube-proxy再做转发，因此效率低、不可靠 |数据包直接在内核态转发，因此效率更高、更可靠|是否修改Client IP |因为kube-proxy在中间做代理，会修改数据包的Client IP| 不修改数据包的Client IPiptables版本和内核支持| 不依赖| iptables &gt; 1.4.11，内核支持route_localnet参数(kernel &gt;= 3.6) 通过设置kube-proxy的启动参数--proxy-mode设定使用userspace还是iptables代理模式。 6. Service发现方式现在服务创建了，得让别人来使用了。别人要使用首先得知道这些服务呀，服务治理很基本的一个功能就是提供服务发现。Kubernetes为我们提供了两种基本的服务发现方式：环境变量和DNS。 6.1 环境变量当一个Pod在节点Node上运行时，kubelet会为每个活动的服务设置一系列的环境变量。它支持Docker links compatible变量，以及更简单的{SVCNAME}_SERVICE_HOST和{SVCNAME}_SERVICE_PORT变量。后者是把服务名字大写，然后把中划线（-）转换为下划线（_）。 以服务redis-master为例，它暴露TCP协议的6379端口，被分配了集群IP地址10.0.0.11，则会创建如下环境变量： REDIS_MASTER_SERVICE_HOST=10.0.0.11REDIS_MASTER_SERVICE_PORT=6379REDIS_MASTER_PORT=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379REDIS_MASTER_PORT_6379_TCP_PROTO=tcpREDIS_MASTER_PORT_6379_TCP_PORT=6379REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 这里有一个注意点是，如果一个Pod要访问一个Service，则必须在该Service之前创建，否则这些环境变量不会被注入此Pod。DNS方式的服务发现就没有此限制。 6.2 DNS虽然DNS是一个cluster add-on特性，但是我们还是强烈推荐使用DNS作为服务发现的方式。DNS服务器通过KubernetesAPI监控新的Service的生成，然后为每个Service设置一堆DNS记录。如果集群设置了DNS，则该集群中所有的Pods都能够使用DNS解析Sercice。 例如，如果在Kubernertes中的my-ns名字空间中有一个服务叫做my-service，则会创建一个my-service.my-ns的DNS记录。在同一个名字空间my-ns的Pods能直接通过服务名my-service查找到该服务。如果是其它的Namespace中的Pods，则需加上名字空间，例如my-service.my-ns。返回的结果是服务的ClusterIP。当然，对于我们上面讲的headless的Service，返回的则是该Service对应的一堆后端Pods的IP地址。 对于知名服务端口，Kubernetes还支持DNS SRV记录。例如my-service.my-ns的服务支持TCP协议的http端口，则你可以通过一个DNS SRV查询_http._tcp.my-service.my-ns来发现http的端口。 对于每个Service的DNS记录，Kubernetes还会加上一个集群域名的后缀，作为完全域名（FQDN）。这个集群域名通过svc+安装集群DNS的DNS_DOMAIN参数指定，默认是svc.cluster.local。如果不是一个标准的Kubernetes支持的安装，则启动kubelet的时候指定参数--cluster-domain，你还需要指定--cluster-dns告诉kubelet集群DNS的地址。 6.3 如何发现和使用服务？一般在创建Pod的时候，指定一个环境变量GET_HOSTS_FROM，值可以设为env或者dns。在Pod中的应用先获取这个环境变量，得到获取服务的方式。如果是env，则通过getenv获取相应的服务的环境变量，例如REDIS_SLAVE_SERVICE_HOST；如果是dns，则可以在Pod内通过标准的gethostbyname获取服务主机名。有个例外是Pod的定义中，不能设置hostNetwork: true。 获取到服务的地址，就可以通过正常方式使用服务了。 如下是Kubernetes自带的guestbook.php中的一段相关代码，供参考： $host = 'redis-slave';if (getenv('GET_HOSTS_FROM') == 'env') &#123; $host = getenv('REDIS_SLAVE_SERVICE_HOST');&#125;$client = new Predis\Client([ 'scheme' =&gt; 'tcp', 'host' =&gt; $host, 'port' =&gt; 6379,]); 7. 一些容器服务中的Service虽然Service在Kubernetes中如此重要，但是对一些基于Kubernetes的容器服务，并没有使用Service，或者用的是上面讨论的headless类型的Service。这种方式基本上是把容器当做VM使用的典型，LoadBalancer和Pods网络互通，通过LoadBalancer直接把流量转发到Pods上，省却了中间由kube-proxy或者iptables的转发方式，从而提高了流量转发效率，但是也由LoadBalancer自己提供对后端Pods的维护，一般需要LoadBalancer提供动态路由的功能（即后端Pods可以动态地从LoadBalancer上注册/注销）。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>iptables</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[容器网络]]></title>
    <url>%2F2018%2F10%2F25%2Fcontainer-network%2F</url>
    <content type="text"><![CDATA[容器网络vxlanvxlan原理: VXLAN通过MAC-in-UDP的报文封装，实现了二层报文在三层网络上的透传,属于overlay网络 Flannel首先，flannel利用Kubernetes-API(这里就是取node.spec.podCIDR)或者etcd用于存储整个集群的网络配置，其中最主要的内容为设置集群的网络地址空间。例如，设定整个集群内所有容器的IP都取自网段“10.1.0.0/16”。 接着，flannel在每个主机中运行flanneld作为agent，它会为所在主机从集群的网络地址空间中，获取一个小的网段subnet，本主机内所有容器的IP地址都将从中分配。 flannel 的 UDP 模式和 Vxlan 模式 host-gw 模式 UDP 模式是 三层 overlay,即，将原始数据包的三层包（IP包）装在 UDP 包里,通过 ip+端口 传到目的地，ip为目标node ip 端口为目标节点上flanneld进程监听的8285端口，解析后传入flannel0设备进入内核网络协议栈，UDP模式下 封包解包是在 flanneld里进行的也就是用户态下 重要！！！ 《深入解析kubernetes》 33章 https://time.geekbang.org/column/article/65287 VxLan 模式 是二层 overlay,即将原始Ethernet包（MAC包）封装起来，通过vtep设备发到目的vtep，vxlan是内核模块，vtep是flannneld创建的，vxlan封包解封完全是在内核态完成的 注意点 inner mac 为 目的vtep的mac outer ip为目的node的ip 这一点和UDP有区别下一跳ip对应的mac地址是ARP表里记录的，inner mac对应的arp记录是 flanneld维护的，outer mac arp表是node自学习的 host-gw 模式的工作原理,是在 节点上加路由表，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。 $ ip route...&lt;目的容器IP地址段&gt; via &lt;网关的IP地址&gt; dev eth0# 网关的 IP 地址，正是目的容器所在宿主机的 IP 地址 Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。如果分布在不同的子网里是不行的，只是三层可达 POD IP的分配使用CNI后，即配置了 kubelet 的 --network-plugin=cni，容器的IP分配：kubelet 先创建pause容器生成network namespace调用 网络driver CNI driverCNI driver 根据配置调用具体的cni 插件cni 插件给pause 容器配置网络pod 中其他的容器都使用 pause 容器的网络 CNM模式Pod IP是docker engine分配的，Pod也是以docker0为网关，通过veth连接network namespace flannel的两种方式 CNI CNM总结CNI中，docker0的ip与Pod无关，Pod总是生成的时候才去动态的申请自己的IP，而CNM模式下，Pod的网段在docker engine启动时就已经决定。CNI只是一个网络接口规范，各种功能都由插件实现，flannel只是插件的一种，而且docker也只是容器载体的一种选择，Kubernetes还可以使用其他的， cluster IP的分配是在kube-apiserver中 `pkg/registry/core/service/ipallocator`中分配的 network policyingress:- from: - namespaceSelector: matchLabels: user: alice - podSelector: matchLabels: role: client 像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系，表示的是yaml数组里的两个元素 ingress:- from: - namespaceSelector: matchLabels: user: alice podSelector: matchLabels: role: client 像上面这样定义的 namespaceSelector 和 podSelector，是“与”（AND）的关系，yaml里表示的是一个数组元素的两个字段 Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。 通过NodePort来访问service的话，client的源ip会被做SNAT client \ ^ \ \ v \ node 1 &lt;--- node 2 | ^ SNAT | | ---&gt; v |endpoint 流程： 客户端发送数据包到 node2:nodePort node2 使用它自己的 IP 地址替换数据包的源 IP 地址（SNAT） node2 使用 pod IP 地址替换数据包的目的 IP 地址 数据包被路由到 node 1，然后交给 endpoint Pod 的回复被路由回 node2 Pod 的回复被发送回给客户端 可以将 service.spec.externalTrafficPolicy 的值为 Local，请求就只会被代理到本地 endpoints 而不会被转发到其它节点。这样就保留了最初的源 IP 地址 不会对访问NodePort的client ip做 SNAT了。如果没有本地 endpoints，发送到这个节点的数据包将会被丢弃。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
        <tag>容器网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务亲和性路由在华为云 k8s 中的实践【Kubernetes中的服务拓扑】]]></title>
    <url>%2F2018%2F07%2F07%2Fservicetopo%2F</url>
    <content type="text"><![CDATA[本文根据华为工程师DJ在LC3上的演讲整理： 1. 拓扑概念首先，我们讲一下kubernetes中的拓扑。根据kubernetes现在的设计，我觉得拓扑可以是任意的。用户可以指定任何拓扑关系，比如az(available zone可用区)、region、机架、主机、交换机，甚至发电机。Kubernetes中拓扑的概念已经在调度器中被广泛使用。 2. Kubernetes调度器中的拓扑在kubernetes中，pod是工作的基本单元，所以调度器的工作可以简化为“在哪里运行这些pod”，当然我们知道pod是运行在节点里，但是怎么选择节点，这是调度器要解决的问题。 在kubernetes中，我们通过label选择节点，从而确定pod应该放在哪个节点中。下面是kubernetes原生提供的一些label： k8s.io/hostname 此外，kubernetes允许集群管理员和云提供商自定义label，比如机架、磁盘类型等。 3.Kubernetes中基于拓扑的调度比如，如果我们要将PostgreSQL服务运行在不同的zone中，假设zone的名字分别是1a和1b，那么我们可以在podSpec中定义节点亲和，如下图所示。主服务器需要运行在zone a的节点中，备用服务器需要运行在属于zone b的节点中。 在kubernetes中，pod与node的亲和或反亲和是刚性的要求。那么我们之前的问题“在哪里运行这些pod”就可以简化成“可以在这个节点运行pod吗”，答案取决于节点的一些情况，比如节点的名字，节点所属的region，节点是否有SSD盘等。 另外，调度器还会考虑另外一个问题，“可以把pod和其他pod放在同一区域吗”，比如，PostgreSQL服务肯定不能和MySQL服务运行在同一个节点中。Kubernetes通过下面三个步骤解决这个问题： 定义“其他pod”。这个过程与选择node的过程类似，我们通过label选择pod。比如，我们可以选择带有“app=web-frontend”label的pod 定义“同一区域”。如图中的两个pod，是在同一区域吗?不是，因为它们podSpec中的topologyKey字段，它是node label中的key，用于指定拓扑区域，比如zone、rack、主机等。比如如果我们使用“k8s.io/hostname”作为topologyKey，那么同一区域就表示在同一个主机上。 我们可以使用之前提到的任何nodelabel作为“同一区域”的标识，比如在同一个zone。或者使用自定义的label，下图给出了通过自定义拓扑label创建node组。 最后是决定是否可以。这取决于亲和和反亲和（affinity/anti-affinity）。 Kubernetes中其他依赖拓扑关系的特性 上面讲到的部署pod时的拓扑感知。除了调度之外，还有许多特性会依赖拓扑关系： 工作负载：在缩容或滚动升级的时候，控制器决定先杀掉哪些pod 卷存储：卷存储会有拓扑限制，来决定可以挂在卷的node集。比如GCE的持久卷只能挂在在同一个zone的节点上，本地卷被所在节点访问。 依赖拓扑的服务亲和性路由1. Service和endpoint首先我们来了解一下kubernetes中的service和endpoint的 概念。 Kubernetes中的service是一个抽象的概念，它通过label选择一个pod的集合，并且定义了这些pod的访问策略。简言之，service指定一个虚拟IP，作为这些pod的访问入口，在4层协议上工作。 Kubernetes中的endpoint是service后端的pod的地址列表。作为使用者，我们不需要感知它们，service创建的时候endpoint会自动创建，并且会根据后端的pod自动配置好。 2. Service的工作原理Endpoints控制器会watch到创建好的service和pod，然后创建相应的endpoint。Kube-proxy会watch service和endpoint，并创建相应的proxy规则。在kubernetes1.8之前proxy是通过底层的iptables实现，但是iptables只支持随机的负载均衡策略，并且可扩展性很差。 在1.8之后，我们实现并在社区持续推动了基于ipvs的proxy，这种proxy模式相对原来的iptables模式有很多优势，比如支持很多负载均衡算法，并且在大规模场景下接近无限扩展性等。好消息是，现在kubernetes社区基于ipvs的proxy已经svc，大家可以在生产环境使用。那么问题来了，既然我们已经有IPVS加持了，为什么还需要服务亲和性路由呢？ 3. 服务亲和性路由先看一下用户的使用场景，当我们将pod正确的放到了用户指定的区域之后，就会有下面的问题。 单节点通信（访问serviceIP的时候只能访问到本节点的应用） 我们使用daemonset部署fluent的时候，应用只需要与当前节点的fluent通信。 一些用户出于安全考虑，希望确保他们只能与本地节点的服务通信，因为跨节点的流量可能会携带来自其他节点的敏感信息。 限制跨zone的流量 因为跨zone的流量会收费，而同一个zone的流量则不会。而有些云提供商，比如阿里云甚至不允许跨zone的流量。 性能优势：显然，到达本区域（节点/zone等）的流量肯定比跨区访问的流量有更低的延时和更高的带宽。 4. 亲和性路由的实现需要解决的问题正如我们前边讲到的，本地意味着一定的拓扑等级，我们需要有一个可以根据拓扑选择endpoint子集的机制。 这样我们就面临着如下问题： 是软亲和还是硬亲和。硬亲和意味着只需要本地的后端，而软亲和意味着首先尝试本地的，如果本地没有则尝试更广范围的。 如果是软亲和，那么判定标准是什么呢？可能给每个拓扑区域增加权重是一个解决方案。 如果多个后端满足条件，那么选择的依据又是什么呢？随机选择还是引入概率？ 5. 我们的方案我们提供了一个解决方案，引用一种新的资源“ServicePolicy”。集群管理员可以通过ServicePolicy配置“local”的选择标准，以及各种拓扑的权重。ServicePolicy是一种可选的namespace范围内的资源，有三种模式：Required/Perferred/Ignored，分别代表硬亲和/软亲和/忽略。 上图是我们引入和ServicePolicy资源和endpoint引入的字段示例。在我们的示例中，ServicePolicy会选择namespace foo中带有label app=bar的service。由于我们将hostname设置为ServicePolicy的拓扑依据，那么对这些service的访问会只路由到与kube-proxy有在同一个host的后端。 需要说明的是，service和ServicePolicy是多对多的关系，一个ServicePolicy可以通过label选择多个service，一个service也可以被多个ServicePolicy选中，有多个亲和性要求。 另外我们还希望endpoint携带节点的拓扑信息，因此我们为endpoint添加一个新的字段Topology，用于识别pod属于的拓扑区域，比如在哪个host/rack/zone/region等。 这会改变现有的逻辑。如下图所示，Endpoint控制器需要watch两种新的资源，node和ServicePolicy，它需要维护node与endpoint的对应关系，并根据node的拓扑信息更新endpoint的Topology字段。另外，kube-proxy也会相应地作一些改动。它需要过滤掉与自身不在同一个拓扑区域的endpoint，这意味着kube-proxy会在不同的节点上创建不同的规则。 下图表示从ServicePolicy到proxy规则的数据流。首先ServicePolicy通过label选择一组service，我们可以根据这些service找到它们的pod。Endpoint控制器会将pod所在节点的拓扑label放到对应的endpoint中。Kube-proxy负责仅为处在同一拓扑区域的endpoint创建proxy规则，并且当多个endpoint满足要求时提供路由策略。 [总 结] 目前我们已经在华为云的CCE服务上实现了服务亲和性路由，效果很好，欢迎大家体验。我们很乐意把这个特性开源出来，并且正在做这件事，相信它会像IPVS一样，成为kubernetes下一个版本的一个重要特性。]]></content>
      <categories>
        <category>k8s</category>
      </categories>
      <tags>
        <tag>k8s</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[了解思科 Tetration 平台]]></title>
    <url>%2F2017%2F06%2F12%2Fcisco-platform%2F</url>
    <content type="text"><![CDATA[介绍 思科推出了 Tetration Analytics平台，Tetration 平台主要是对大规模数据中心和云平台上的网络流量的实时采集、存储和分析。 Tetration 平台搭配基于Cloud Scale技术的硬件设备，流动在数据中心的任何一个数据包的元信息都可以被实时记录和存储下来。 Tetration 平台可以辅助用户在应用关系梳理、应用访问策略制定、模拟和实时验证、应用云端迁移访问策略制定、白名单安全模型等方面脱离传统手工和被动的工作方式。 Tetration Analytics平台主要由 数据采集部分、数据存储部分、和数据分析部分组成。 数据采集部分：包括安装在实体服务器或者虚拟机中的软件数据采集器、以太网交换机转发芯片的硬件数据采集逻辑和第三方数据接口组成。软件数据采集器通过 libpcap（一个网络数据包捕获函数库，linux抓包工具tcpdump就是基于此的）来对数据进行采集。 存储和分析部分：由基于思科UCS计算平台的服务器集群组成。 下图是思科Tetration Analytics平台架构 存储和分析部分是该平台的精髓所在，针对万亿个数据的无监督机器学习算法的采用，为网络访问行为基线设立、网络访问异常检测、应用访问关系的动态甄别、聚类动态划分等提供了方便的工具。平台为用户提供了网络数据完善的、全面的大数据来源。 Tetration Analytics平台提供了存储和分析的接口，用户可以根据数据进行相应的网络数据分析，提供的接口包括 开放式 API、REST、推送事件、用户应用 平台特性思科 Tetration Analytics 能够分析应用行为，并准确地反映出应用之间的依赖关系。它采用机器学习技术构建动态分层策略模型，从而实现应用分段和自动策略实施 Tetration Analytics 可大幅简化零信任模式的实施。它可以针对数据中心内的任何对象实时提供可视性。它使用基于行为的应用洞察和机器学习技术来构建动态策略模型，实现自动策略实施。此外，它还通过 REST API 支持开放式访问，客户可以编写个性化应用。 遥感勘测快上加快 Tetration Analytics 使用无需监管的机器学习技术，以线速处理收集的遥感勘测数据。借助自然语言技术，搜索和浏览数百亿条数据流记录。只需不到一秒即可获得切实可行的见解。 切实可行的应用见解 依据应用组件和行为分析算法获取实时数据。标识应用组及其通信模式和服务依赖性。获取自动化白名单策略建议，实现零信任安全性。 应用分段 在本地数据中心以及公共云和私有云中实施一致的策略，实现应用分段。持续监控合规性偏差，可在几分钟内发现生产网络中的违规情况。 开放式 API 利用全面精细的遥感勘测数据，轻松打造个性化的定制应用。生成个性化的定制通知和查询。监控应用层的延迟情况并获取通知。此平台使用 REST API。 强大的可扩展性 Tetration Analytics 从数据中心的每个数据包收集遥感勘测数据。它可以在几秒钟内分析数百万个事件并从数十亿条记录中提供切实可行的见解。它可以长期保留数据，而不会丢失细节。ddd]]></content>
      <categories>
        <category>数据中心</category>
      </categories>
      <tags>
        <tag> Analytics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 介绍及其应用]]></title>
    <url>%2F2017%2F06%2F11%2FDocker%2F</url>
    <content type="text"><![CDATA[重要链接！！！知乎上的深入浅出 1.Docker 介绍便于入题，首先用 Docker 的logo解释下： 那个大鲸鱼（或者是货轮）就是操作系统 把要交付的应用程序看成是各种货物，原本要将各种各样形状、尺寸不同的货物放到大鲸鱼上，你得为每件货物考虑怎么安放（就是应用程序配套的环境），还得考虑货物和货物是否能叠起来（应用程序依赖的环境是否会冲突）。 现在使用了集装箱（容器）把每件货物都放到集装箱里，这样大鲸鱼可以用同样地方式安放、堆叠集装了，省事省力。参考知乎 步入正题： Docker 是 PaaS 提供商 dotCloud 开源的一个基于 LXC 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。 Docker可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。 如图所示，Docker 使用客户端-服务器 (C/S) 架构模式。 Docker 客户端会与 Docker 守护进程进行通信。 Docker 守护进程会处理复杂繁重的任务，例如建立、运行、发布你的 Docker 容器。 Docker 客户端和守护进程 Daemon 可以运行在同一个系统上，当然你也可以使用 Docker 客户端去连接一个远程的 Docker 守护进程。Docker 客户端和守护进程之间通过 socket 或者 RESTful API 进行通信，就像下图。 1.1 Docker 守护进程如上图所示，Docker 守护进程运行在一台主机上。用户并不直接和守护进程进行交互，而是通过 Docker 客户端间接和其通信。 1.2 Docker 客户端Docker 客户端，实际上是 docker 的二进制程序，是主要的用户与 Docker 交互方式。它接收用户指令并且与背后的 Docker 守护进程通信，如此来回往复。 1.3 Docker 内部要理解 Docker 内部构建，需要理解以下三种部件： Docker 镜像 - Docker imagesDocker 仓库 - Docker registeriesDocker 容器 - Docker containers Docker 镜像:Docker 镜像是 Docker 容器运行时的只读模板，每一个镜像由一系列的层 (layers) 组成。Docker 使用 UnionFS 来将这些层联合到单独的镜像中。UnionFS 允许独立文件系统中的文件和文件夹(称之为分支)被透明覆盖，形成一个单独连贯的文件系统。正因为有了这些层的存在，Docker 是如此的轻量。当你改变了一个 Docker 镜像，比如升级到某个程序到新的版本，一个新的层会被创建。因此，不用替换整个原先的镜像或者重新建立(在使用虚拟机的时候你可能会这么做)，只是一个新 的层被添加或升级了。现在你不用重新发布整个镜像，只需要升级，层使得分发 Docker 镜像变得简单和快速。 Docker 仓库:Docker 仓库用来保存镜像，可以理解为代码控制中的代码仓库。同样的，Docker 仓库也有公有和私有的概念。公有的 Docker 仓库名字是 Docker Hub。Docker Hub 提供了庞大的镜像集合供使用。这些镜像可以是自己创建，或者在别人的镜像基础上创建。Docker 仓库是 Docker 的分发部分。 Docker 容器:Docker 容器和文件夹很类似，一个Docker容器包含了所有的某个应用运行所需要的环境。每一个 Docker 容器都是从 Docker 镜像创建的。Docker 容器可以运行、开始、停止、移动和删除。每一个 Docker 容器都是独立和安全的应用平台，Docker 容器是 Docker 的运行部分。 2. Docker 8个的应用场景 本小节介绍了常用的8个Docker的真实使用场景，分别是简化配置、代码流水线管理、提高开发效率、隔离应用、整合服务器、调试能力、多租户环境、快速部署 一些Docker的使用场景，它为你展示了如何借助Docker的优势，在低开销的情况下，打造一个一致性的环境。 1.简化配置这是Docker公司宣传的Docker的主要使用场景。虚拟机的最大好处是能在你的硬件设施上运行各种配置不一样的平台（软件、系统），Docker在降低额外开销的情况下提供了同样的功能。它能让你将运行环境和配置放在代码中然后部署，同一个Docker的配置可以在不同的环境中使用，这样就降低了硬件要求和应用环境之间耦合度。 2. 代码流水线（Code Pipeline）管理前一个场景对于管理代码的流水线起到了很大的帮助。代码从开发者的机器到最终在生产环境上的部署，需要经过很多的中间环境。而每一个中间环境都有自己微小的差别，Docker给应用提供了一个从开发到上线均一致的环境，让代码的流水线变得简单不少。 3. 提高开发效率不同的开发环境中，我们都想把两件事做好。一是我们想让开发环境尽量贴近生产环境，二是我们想快速搭建开发环境。 理想状态中，要达到第一个目标，我们需要将每一个服务都跑在独立的虚拟机中以便监控生产环境中服务的运行状态。然而，我们却不想每次都需要网络连接，每次重新编译的时候远程连接上去特别麻烦。这就是Docker做的特别好的地方，开发环境的机器通常内存比较小，之前使用虚拟的时候，我们经常需要为开发环境的机器加内存，而现在Docker可以轻易的让几十个服务在Docker中跑起来。 4. 隔离应用有很多种原因会让你选择在一个机器上运行不同的应用，比如之前提到的提高开发效率的场景等。 我们经常需要考虑两点，一是因为要降低成本而进行服务器整合，二是将一个整体式的应用拆分成松耦合的单个服务（译者注：微服务架构）。 5. 整合服务器正如通过虚拟机来整合多个应用，Docker隔离应用的能力使得Docker可以整合多个服务器以降低成本。由于没有多个操作系统的内存占用，以及能在多个实例之间共享没有使用的内存，Docker可以比虚拟机提供更好的服务器整合解决方案。 6. 调试能力Docker提供了很多的工具，这些工具不一定只是针对容器，但是却适用于容器。它们提供了很多的功能，包括可以为容器设置检查点、设置版本和查看两个容器之间的差别，这些特性可以帮助调试Bug。 7. 多租户环境另外一个Docker有意思的使用场景是在多租户的应用中，它可以避免关键应用的重写。我们一个特别的关于这个场景的例子是为IoT（物联网）的应用开发一个快速、易用的多租户环境。这种多租户的基本代码非常复杂，很难处理，重新规划这样一个应用不但消耗时间，也浪费金钱。 使用Docker，可以为每一个租户的应用层的多个实例创建隔离的环境，这不仅简单而且成本低廉，当然这一切得益于Docker环境的启动速度和其高效的diff命令。 8. 快速部署在虚拟机之前，引入新的硬件资源需要消耗几天的时间。虚拟化技术（Virtualization）将这个时间缩短到了分钟级别。而Docker通过为进程仅仅创建一个容器而无需启动一个操作系统，再次将这个过程缩短到了秒级。这正是Google和Facebook都看重的特性。 你可以在数据中心创建销毁资源而无需担心重新启动带来的开销。通常数据中心的资源利用率只有30%，通过使用Docker并进行有效的资源分配可以提高资源的利用率。 本文的几个概念 LXC: LXC为Linux Container的简写。可以提供轻量级的虚拟化，以便隔离进程和资源，而且不需要提供指令解释机制以及全虚拟化的其他复杂性。属于操作系统层次之上的虚拟化]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 中的并发]]></title>
    <url>%2F2017%2F06%2F10%2Fjava-create-thread%2F</url>
    <content type="text"><![CDATA[Java 中的并发如何创建一个线程按 Java 语言规范中的说法，创建线程只有一种方式，就是创建一个 Thread 对象。而从 HotSpot 虚拟机的角度看，创建一个虚拟机线程 有两种方式，一种是创建 Thread 对象，另一种是创建 一个本地线程，加入到虚拟机线程中。 如果从 Java 语法的角度。有两种方法。 第一是继承 Thread 类，实现 run 方法，并创建子类对象。 public void startThreadUseSubClass() &#123; class MyThread extends Thread &#123; public void run() &#123; System.out.println("start thread using Subclass of Thread"); &#125; &#125; MyThread thread = new MyThread(); thread.start();&#125; 另一种是传递给 Thread 构造函数一个 Runnable 对象。 public void startThreadUseRunnalbe() &#123; Thread thread = new Thread(new Runnable() &#123; public void run() &#123; System.out.println("start thread using runnable"); &#125; &#125;); thread.start();&#125; 当然， Runnalbe 对象，也不是只有这一种形式，例如如果我们想要线程执行时返回一个值，就需要用到另一种 Runnalbe 对象，它 对原来的 Runnalbe 对象进行了包装。 public void startFutureTask() &#123; FutureTask&lt;Integer&gt; task = new FutureTask&lt;&gt;(new Callable&lt;Integer&gt;() &#123; public Integer call() &#123; return 1; &#125; &#125;); new Thread(task).start(); try &#123; Integer result = task.get(); System.out.println("future result " + result); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 结束线程wait 与 sleepsleep 会使得当前线程休眠一段时间，但并不会释放已经得到的锁。 wait 会阻塞住，并释放已经得到的锁。一直到有人调用 notify 或者 notifyAll，它会重新尝试得到锁，然后再唤醒。 线程池好处 复用 线程池中有一系列线程，这些线程在执行完任务后，并不会被销毁，而会从任务队列中取出任务，执行这些任务。这样，就避免为每个任务 都创建线程，销毁线程。 在有大量短命线程的场景下，如果创建线程和销毁线程的时间比线程执行任务的时间还长，显然是不划算的，这时候，使用线程池就会有明显 的好处。 流控 同时，可以设置线程数目，这样，线程不会增大到影响系统整体性能的程度。当任务太多时，可以在队列中排队， 如果有空闲线程，他们会从队列中取出任务执行。 使用 线程数目 那么，线程的数目要设置成多少呢？这需要根据任务类型的不同来设置，假如是大量计算型的任务，他们不会阻塞，那么可以将线程数目设置 为处理器数目。而如果任务中涉及大量IO，有些线程会阻塞住，这样就要根据阻塞线程数目与运行线程数目的比例，以及处理器数目来设置 线程总数目。例如阻塞线程数目与运行线程数目之比为n, 处理器数目为p，那么可以设置 n * (p + 1) 个线程，保证有 n 个线程处于运行 状态。 Executors JDK 的 java.util.concurrent.Executors 类提供了几个静态的方法，用于创建不同类型的线程池。 ExecutorService service = Executors.newFixedThreadPool(10);ArrayList&lt;Future&lt;Integer&gt;&gt; results = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 14; i++) &#123; Future&lt;Integer&gt; r = service.submit(new Callable&lt;Integer&gt;() &#123; public Integer call() &#123; return new Random().nextInt(); &#125;); results.add(r);&#125; newFixedThreadPool 可以创建固定数目的线程，一旦创建不会自动销毁线程，即便长期没有任务。除非显式关闭线程池。如果任务队列中有任务，就取出任务执行。 另外，还可以使用 newCachedThreadPool 方法创建一个不设定固定线程数目的线程池，它有一个特性，线程完成任务后，如果一分钟之内又有新任务，就会复用这个线程执行新任务。如果超过一分钟还没有任务执行，就会自动销毁。 另外，还提供了 newSingleThreadExecutor 创建有一个工作线程的线程池。 原理JDK 中的线程池通过 HashSet 存储工作者线程，通过 BlockingQueue 来存储待处理任务。 通过核心工作者数目(corePoolSize) 和 最大工作者数目(maximumPoolSize) 来确定如何处理任务。如果当前工作者线程数目 小于核心工作者数目，则创建一个工作者线程执行这个任务。否则，将这个任务放入待处理队列。如果入队失败，再看看当前工作 者数目是不是小于最大工作者数目，如果小于，则创建工作者线程执行这个任务。否则，拒绝执行这个任务。 另外，如果待处理队列中没有任务要处理，并且工作者线程数目超过了核心工作者数目，那么，需要减少工作者线程数目。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>线程</tag>
      </tags>
  </entry>
</search>
